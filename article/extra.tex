\section{Extra}

%The problem with evaluating preprocessing pipelines is, that we lack ground truth.
%Therefore any approach learning is potentially prone to overfitting.
%with our novel workflow or with the Here we derive two novel metrics for assessing the quality of the registration, which were not used by engineering the new workflow nor used in the optimisation procedure. 

It is significantly more time-consuming and primarily relevant as a demonstration rather than an efficient way to test, analyze, and incrementally improve preprocessing workflows.

The templates used for mouse brain MRI are highly heterogeneous, limiting data integration potential.
Affine “removal” comes to the strong detriment to the resulting neuroimaging data, which persists in all downstream statistics produced from such data.
This is due to the fact that visual representation unavoidably requires an affine transformation (to turn data points into volumes in space), and the deletion of this information makes the spatial representation more rather than less ambiguous.
One common issue which may arise is that software (rightfully) attempts to recreate the affine information.
Such data recovery behaviour is not determined by the NIfTI (or any other) standard, and as such is unpredictable.
An illustration of this issue ls given by the comparison of coordinates in the MRIcroGL and SAMRI (internally using NiLearn) plots of the legacy pipeline results.

While a population template may require less deformation, it only permits within-study comparability, and requires additional interpolation in order to allow region of interest (ROI) delineation or inter-study comparison.
While it is beyond the scope or intent of this study to single out and individually investigate articles by our colleagues, we offer a comparison from our own data and pipelines, illustrating the benefits of context awareness and specific optimizations in animal MRI.
Destructive hacks in particular (such as affine transformation deletion or scaling) preclude both the usage of preprocessed data and of the preprocessing workflow itself in size-sensitive applications, which may include a plethora of diagnostic or preoperative imaging scenarios (e.g. !!!cite StereotaXYZ!!!).

Human MRI research has produced numerous registration toolkits and associated workflow implementations, predominantly accessed via high-level interfaces contain hard-coded parameters optimized for specific human MRI use cases.
Animal MRI commonly makes use of these high-level interfaces, and implements additional hacks to mitigate the nonhuman idiosyncracies of the species being imaged --- instead of optimizing the workflow for the data at hand.
Quality control is commonly performed by operator inspection, making it infrequent, biased, slow, and unreproducible.
In this paper we present a novel workflow using the full flexibility of low-level functions from one of the most popular neuroimaging registration toolkits, and provide an optimized set of parameters for small animal imaging.
Additionally, we present a quality control (QC) workflow, which can automatically assess the registration quality of processed datasets.
We showcase the capabilities of both workflow, by comparing our current registration performance with that of a legacy registration workflow (containing multiple popular hacks - which we specifically critique).

\subsection{Variance Analysis}

While the absolute value of image similarity metrics cannot be relied upon for QC, the variance structure of similarity metrics in longitudinal datasets can.
The rationale of analysing similarity metric variance is that in healthy adult mice, there is no significant change in brain structure over time, but there are significant differences in individual anatomy.
This proposition is further strengthened in our dataset, as each animal has a slightly different but temporally stable implant placement. 
The corollary of this statement in terms of value distributions, is that the more variance is reduced across sessions without being reduced across subjects, the more stable a workflow is and the less likely it is for its performance to be derived from over-fitting.


We use this metric to optimize our registration parameters, as it is uniquely suites to our dataset (as opposed to to other implant-less data sets), and allows us to leverage it to a maximum use for the community, while keeping other less intricate and unrelated metrics usable for transferable QC for external datasets.


\[ CC(x) = \frac{\sum_i{(x)}}{\sum_i{(x)}} \]

%\py{pytex_fig('scripts/registration_qc.py', conf='article/varplot.conf', label='varplot', caption='Variance for different preprocessing pipelines')}

To assess the quality of the pipeline we evaluated the registration performance for different metrices (Crosscorrelation (CC), Mutual Information (MI), Mean Squared Difference (MSQ)) for individual sessions and subjects on a representatitve dataset.
We define an assessment for registration quality based on the assumption, that for increased registration quality the variance of a similariy measure between the subject and the template should converge towards 0.
This definition is based on the assumption that biological deformations of the brain across sessions should be negligible (Ref???!?!).
Hence we calculate the variance over different similiarity metrics for each subject across sessions.
We average for each workflow the results across subjects.
We find that our new preprocessing pipeline has significantly less variance than the legacy workflow, while the optimised pipeline has even further decreased variance.

\subsection{Functional Feature Mapping}

Distinct functional features, if available, can be tracked to provide QC supplementing the analysis of overall statistical power (as estimated by MS).
This method is even more restricted in general applicability than the global functional analysis, as it requires an a priori known feature which is easily distinguished via statistical estimates from its immediate surroundings.
The data set at hand, however, presents such a feature in the form of the DR, which is targeted for stimulation and know a priori (cite Joanes!!!) to present activation, contrasting with concomitant deactivation in other areas of the brain. 
To ensure sensitivity to the directionality of the response we analyze the per-animal mean t-statistic in the DR.

\begin{sansmath}
\py{pytex_subfigs(
        [
                {'script':'scripts/fdrtc_violin.py', 'label':'facv','conf':'article/1col.conf', 'options_pre':'{.48\\textwidth}',
                        'caption':'Comparison across workflows and functional contrasts, considering only matching template-workflow combinations.'
                        ,},
                {'script':'scripts/fdrt_violin.py', 'label':'fav', 'conf':'article/1col.conf', 'options_pre':'{.48\\textwidth}',
			'caption':'Comparison across workflows and target templates, considering only CBV functional contrasts.'
                        ,},
                ],
        caption='Mean per-animal t-value in Dorsal Raphe. Coloured patch width estimates distribution density, while continuous markers indicate the sample mean and dashed markers indicate the inner quartiles.',
        label='fig:fa',)}
\end{sansmath}

A salient feature seen in this comparison, is that With respect to the data break-up by contrast (from \cref{fig:facv}), we see a notable positive main effect for the contrast variable
(Dorsal Raphe t-statistic mean of
\py{boilerplate.corecomparison_factorci('Contrast[T.CBV]', df_path='data/functional_t.csv', dependent_variable='Mean DR t')}
).
This encourages a qualitative analysis of the statistical distribution maps, which we present in \cref{fig:m}.

As seen in \cref{fig:fa}, we note that functional score is sensitive to neither
the processing workflow
(\py{boilerplate.fstatistic('Processing', dependent_variable='Mean DR t', df_path='data/functional_t.csv', condensed=True)}),
nor the template
(\py{boilerplate.fstatistic('Template', dependent_variable='Mean DR t', df_path='data/functional_t.csv', condensed=True)}),
nor the interaction thereof
(\py{boilerplate.fstatistic('Processing:Template', dependent_variable='Mean DR t', df_path='data/functional_t.csv', condensed=True)}).

Testing the core hypothesis of the comparison ---
whether the Generic SAMRI workflow (with the Generic template) performs significantly different than the Legacy workflow (with the Legacy template) ---
we note that it does not
(two-tailed p-value of
\py{pytex_printonly('scripts/fdrs_t.py')}).
As before, we note that there is a variance increase in all conditions for the Legacy processing workflow
(\py{boilerplate.varianceratio(df_path='data/functional_t.csv', template='Legacy', dependent_variable='Mean DR t')}-fold
given the Legacy template, and
\py{boilerplate.varianceratio(template='Generic')}-fold
given the Generic template).
